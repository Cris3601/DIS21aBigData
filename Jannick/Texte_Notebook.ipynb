{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad40767",
   "metadata": {},
   "source": [
    "# Pre-trained CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d5a28",
   "metadata": {},
   "source": [
    "## Pre-trained CNN\n",
    "\n",
    "### Einleitung\n",
    "\n",
    "- HAT BENE BEREITS GESCHRIEBEN\n",
    "- Mit diesem Model passen wir die Gewichte an unser Model an.\n",
    "\n",
    "### Ergebnis\n",
    "\n",
    "Durch die bereits vortrainierte Convolution Basis und das zusätzlich Training in den letzten (N Epochen) mit unserem Model, ist die conv_base (Convolutional Basis) jetzt bereit für den nächsten Schritt. Nämlich Data Augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437dbcc",
   "metadata": {},
   "source": [
    "## Pre-trained CNN - Feature Extraction - data augmentation\n",
    "\n",
    "### Einleitung\n",
    "\n",
    "Ähnlich wie bereit bei dem nicht trainiertem Modell aus (Unterpunkt X), ist Data Augmentation sehr rechenintensiv, aber hoffentlich die zusätzliche Investition wert. Um die bereits trainierten Gewichte nicht zu verlieren, müssen die Gewichte „eingefrohren“ werden. Dazu wird das Model mit einem Dense Layer erweitert, welche als Classifier fungiert.\n",
    "\n",
    "### Ergebnis\n",
    "\n",
    "- Durch die zusätzlich generierten Bilder der Data Augmentation und der auf unser Modell trainierten conv_base haben wir eine bessere Val_Acc erhalten: XXX. Jedoch ist Overfitting ein weiteres, wenn auch geringeres Problem. \n",
    "- Bevor wir uns dem Fine Tuning der letzten Schichten der conv_basis zuwenden, haben wir beschlossen ein Hyperparameter Tuning durchzuführen. (Model bauen aufeinander auf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a28439",
   "metadata": {},
   "source": [
    "## Pre-trained CNN - Feature Extraction - data augmentation - hyperparameter tuning\n",
    "\n",
    "### Einleitung\n",
    "\n",
    "Mit einem Hyperparameter Tuning möchten wir die Eigenschaften unseres Modells (und der conv_base) für weitere Analyseschritte optimieren. Unser Testplan ist im Dictionary XXX vorhanden.\n",
    "\n",
    "### Ergebnis\n",
    "\n",
    "Nach dem Hyperparameter Tuning haben wir unsere besten Ergebnisse mit folgenden Parametern gehabt: learning_rate = xx, dropout = xx, weight = xx. Jetzt können wir mit dem finalen Schritt beginnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060b0f1",
   "metadata": {},
   "source": [
    "## Pre-trained CNN - Feature Extraction - Fine Tuning\n",
    "\n",
    "### Einleitung\n",
    "\n",
    "Zuletzt führen wir ein sogenanntes „Fine Tuning“ durch. Durch das „entfreezen“ einiger der letzten Layer erwarten wir uns eine erneute Verbesserung bezüglich Validation Loss und Validation Accuracy. Sozusagen möchten wir den letzte Feinschliff an das Model bringen. \n",
    "\n",
    "### Ergebnis\n",
    "\n",
    "Wie durch unser Ergebnis zu sehen ist, hat sich der Analyse-Aufwand gelohnt. Auch wenn unser Modell keine 95-100% erreicht hat, sind wir mit XX% gut mit dabei. Ein besseres Ergebnis sind wahrscheinlich aufgrund der Unterschiedlichen Qualität der Bilder und wenigen „Bild-Detail“ nur schwer möglich. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f353e7",
   "metadata": {},
   "source": [
    "## Allgemein Notizen:\n",
    "- bei mir in DIS21a - Notizen.docx (local auf dem Rechner)\n",
    "- unter anderm mögliche Struktur & Ideen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_plan dictionary\n",
    "test_plan = {}\n",
    "test_plan[0] = {\"n_units\": 256, \"learning_rate\": 2e-5, \"batch_size\": 20}\n",
    "test_plan[1] = {\"n_units\": 128, \"learning_rate\": 2e-5, \"batch_size\": 20}\n",
    "test_plan[2] = {\"n_units\": 512, \"learning_rate\": 2e-5, \"batch_size\": 20}\n",
    "test_plan[3] = {\"n_units\": 256, \"learning_rate\": 2e-4, \"batch_size\": 20}\n",
    "test_plan[4] = {\"n_units\": 256, \"learning_rate\": 2e-3, \"batch_size\": 20}\n",
    "test_plan[5] = {\"n_units\": 256, \"learning_rate\": 2e-5, \"batch_size\": 40}\n",
    "test_plan[6] = {\"n_units\": 256, \"learning_rate\": 2e-5, \"batch_size\": 60}\n",
    "test_plan[7] = {\"learning Rate\": 1e-5, \"dropout\": 0, \"weight regularization\": 0}\n",
    "test_plan[8] = {\"learning Rate\": 1e-6, \"dropout\": 0, \"weight regularization\": 0}\n",
    "test_plan[9] = {\"learning Rate\": 1e-7, \"dropout\": 0, \"weight regularization\": 0}\n",
    "test_plan[10] = {\"learning Rate\": 1e-5, \"dropout\": 0.4, \"weight regularization\": 0}\n",
    "test_plan[11] = {\"learning Rate\": 1e-5, \"dropout\": 0.6, \"weight regularization\": 0}\n",
    "test_plan[12] = {\"learning Rate\": 1e-5, \"dropout\": 0, \"weight regularization\": 1e-3}\n",
    "test_plan[13] = {\"learning Rate\": 1e-5, \"dropout\": 0, \"weight regularization\": 1e-2}\n",
    "test_plan[14] = {\"learning Rate\": 1e-5, \"dropout\": 0, \"weight regularization\": 1e-1}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
