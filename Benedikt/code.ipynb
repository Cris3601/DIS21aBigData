{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bene\\AppData\\Local\\Temp/ipykernel_17288/1246064730.py:27: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "path = '../data/big/benign'\n",
    "for file in os.listdir(path):\n",
    "    new = 'benign.' + file\n",
    "    os.rename(os.path.join(path, file), os.path.join(path, new))\n",
    "\n",
    "path = '../data/big/malignant'\n",
    "for file in os.listdir(path):\n",
    "    new = 'malignant.' + file\n",
    "    os.rename(os.path.join(path, file), os.path.join(path, new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dir= ../data/idea1\\train\n",
      "validation_dir= ../data/idea1\\validation\n",
      "test_dir= ../data/idea1\\test\n"
     ]
    }
   ],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset is uncompressed\n",
    "original_dataset_dir = '../data/big'\n",
    "\n",
    "# The directory where to\n",
    "# store the smaller dataset\n",
    "base_dir = '../data/idea1'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "# Create the directories for the \n",
    "# training partition\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    \n",
    "# validation partition\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n",
    "\n",
    "# test partition\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "print(\"train_dir=\",train_dir)\n",
    "print(\"validation_dir=\",validation_dir)\n",
    "print(\"test_dir=\",test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with the training benign pictures\n",
    "train_benign_dir = os.path.join(train_dir, 'benign')\n",
    "if not os.path.exists(train_benign_dir):\n",
    "    os.mkdir(train_benign_dir)\n",
    "\n",
    "# Directory with the training dog pictures\n",
    "train_malignant_dir = os.path.join(train_dir, 'malignant')\n",
    "if not os.path.exists(train_malignant_dir):\n",
    "    os.mkdir(train_malignant_dir)\n",
    "\n",
    "# Directory with the validation benign pictures\n",
    "validation_benign_dir = os.path.join(validation_dir, 'benign')\n",
    "if not os.path.exists(validation_benign_dir):\n",
    "    os.mkdir(validation_benign_dir)\n",
    "\n",
    "# Directory with the validation dog pictures\n",
    "validation_malignant_dir = os.path.join(validation_dir, 'malignant')\n",
    "if not os.path.exists(validation_malignant_dir):\n",
    "    os.mkdir(validation_malignant_dir)\n",
    "\n",
    "# Directory with the test benign pictures\n",
    "test_benign_dir = os.path.join(test_dir, 'benign')\n",
    "if not os.path.exists(test_benign_dir):\n",
    "    os.mkdir(test_benign_dir)\n",
    "\n",
    "# Directory with the test dog pictures\n",
    "test_malignant_dir = os.path.join(test_dir, 'malignant')\n",
    "if not os.path.exists(test_malignant_dir):\n",
    "    os.mkdir(test_malignant_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy first 1000 benign images to train_benigns_dir\n",
    "fnames = ['benign.{}.jpg'.format(i) for i in range(1,1000)]\n",
    "for fname in fnames:\n",
    "    src = original_dataset_dir + \"/\" + fname\n",
    "    dst = train_benign_dir + \"/\" + fname\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy NEXT 250 benign images to validation_benigns_dir\n",
    "fnames = ['benign.{}.jpg'.format(i) for i in range(1000, 1250)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_benign_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy NEXT 250 benign images to test_benign_dir\n",
    "fnames = ['benign.{}.jpg'.format(i) for i in range(1250, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_benign_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 malignant images to train_malignant_dir\n",
    "fnames = ['malignant.{}.jpg'.format(i) for i in range(1,1000)]\n",
    "for fname in fnames:\n",
    "    try: \n",
    "        src = os.path.join(original_dataset_dir, fname)\n",
    "        dst = os.path.join(train_malignant_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# Copy next 500 malignant images to validation_malignant_dir\n",
    "fnames = ['malignant.{}.jpg'.format(i) for i in range(1000, 1250)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_malignant_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 malignant images to test_malignant_dir\n",
    "fnames = ['malignant.{}.jpg'.format(i) for i in range(1250, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_malignant_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1995 images belonging to 2 classes.\n",
      "Found 500 images belonging to 2 classes.\n",
      "<class 'keras.preprocessing.image.DirectoryIterator'>\n"
     ]
    }
   ],
   "source": [
    "# richtiges coding\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(224, 244, 3)))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# All images will be [0,1] standardized\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "training_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(224, 244),\n",
    "        batch_size=20,\n",
    "        # Since binary_crossentropy loss is used, binary labels are needed\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(224, 244),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "asdf = [1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "asdf[0:4]\n",
    "\n",
    "print(type(training_generator))\n",
    "# print(validation_generator[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "benign_input = \"../data/only_pictures/benign\"\n",
    "malignant_input = \"../data/only_pictures/malignant\"\n",
    "\n",
    "train_data_loc = \"../data/only_pictures/benign\"\n",
    "\n",
    "benign_generator = train_datagen.flow_from_directory(\n",
    "    # This is the target directory\n",
    "    train_data_loc,\n",
    "    # All images will be resized to 150x150\n",
    "    target_size=(224, 224),\n",
    "    # Since binary_crossentropy loss is used, binary labels are needed\n",
    "    class_mode='binary')\n",
    "\n",
    "malignant_generator = train_datagen.flow_from_directory(\n",
    "    # This is the target directory\n",
    "    malignant_input,\n",
    "    # All images will be resized to 150x150\n",
    "    target_size=(224, 224),\n",
    "    # Since binary_crossentropy loss is used, binary labels are needed\n",
    "    class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3297 images belonging to 2 classes.\n",
      "the number of pictures in this dic is: 3297\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bene\\AppData\\Local\\Temp/ipykernel_17288/306576687.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  numpy_training_data = np.array(training_data)\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"../data/only_pictures\"\n",
    "training_generator = train_datagen.flow_from_directory(\n",
    "    # This is the target directory\n",
    "    train_dir,\n",
    "    # All images will be resized to 150x150\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    # Since binary_crossentropy loss is used, binary labels are needed\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "\n",
    "training_data = []\n",
    "\n",
    "# malignant\n",
    "max_value = (len(os.listdir(train_dir+\"/benign\")) + len(os.listdir(train_dir+\"/malignant\")))\n",
    "print(\"the number of pictures in this dic is:\",max_value)\n",
    "for index, x in enumerate(training_generator):\n",
    "    \n",
    "    # safe label info as integer\n",
    "    if(1 in x[1] ):\n",
    "        temp_to_add = 1\n",
    "    else:\n",
    "        temp_to_add = 0\n",
    "\n",
    "    # ingore the batch size and only take the first element of a list with one element\n",
    "    for i in x[0]:\n",
    "        training_data.append([i, temp_to_add])\n",
    "\n",
    "    if(index % max_value == 0 and index != 0):\n",
    "        break\n",
    "    if(index % 100 == 0):\n",
    "        print(index)\n",
    "numpy_training_data = np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[3200])\n",
    "\n",
    "# ebene 1 is the number of pictures \n",
    "# ebene 2 is malignant or benign\n",
    "# ebene 3 is the batch size - this makes problems\n",
    "# ebene 4 is the picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(training_data[1997])\n",
    "\n",
    "# shows if malignant or benign\n",
    "# is one\n",
    "print(training_data[111][1])\n",
    "# is zero\n",
    "print(training_data[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = []\n",
    "\n",
    "for i in training_data:\n",
    "    temp_think = {}\n",
    "    temp_think[\"label\"] = i[1]\n",
    "    temp_think[\"image\"] = i[0]\n",
    "    df.append(temp_think)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.45098042, 0.20392159, 0.08235294], [0.729...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.81568635, 0.74509805, 0.8470589], [0.8156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.69411767, 0.5019608, 0.48627454], [0.6862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.8352942, 0.5019608, 0.4901961], [0.839215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.26666668, 0.227451, 0.27058825], [0.28627...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              image\n",
       "0      1  [[[0.45098042, 0.20392159, 0.08235294], [0.729...\n",
       "1      0  [[[0.81568635, 0.74509805, 0.8470589], [0.8156...\n",
       "2      0  [[[0.69411767, 0.5019608, 0.48627454], [0.6862...\n",
       "3      0  [[[0.8352942, 0.5019608, 0.4901961], [0.839215...\n",
       "4      1  [[[0.26666668, 0.227451, 0.27058825], [0.28627..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "# convert from list with 2 elements to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bene\\AppData\\Local\\Temp/ipykernel_17288/414996032.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp_train = np.array(training_data)[train_index]\n",
      "C:\\Users\\Bene\\AppData\\Local\\Temp/ipykernel_17288/414996032.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp_validate = np.array(training_data)[test_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2968 2968\n",
      "446767104 2968\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 224, 244, 3), found shape=(None, 224, 224, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17288/414996032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Train the model on the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Evaluate the model on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 224, 244, 3), found shape=(None, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds\n",
    "k = 10\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=k)\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop over the folds\n",
    "for train_index, test_index in kf.split(training_data):\n",
    "    # Get the training and test sets for this fold\n",
    "    \n",
    "    temp_train = np.array(training_data)[train_index]\n",
    "\n",
    "    temp_validate = np.array(training_data)[test_index]\n",
    "    # y_train, y_test = training_data[train_index][0], training_data[test_index][0]\n",
    "    \n",
    "    # temp_train_dir_x = os.listdir(train_dir+\"/benign\")[train_index[0]:train_index[-1]]\n",
    "    # temp_train_dir_x.extend(os.listdir(train_dir+\"/malignant\")[train_index[0]:train_index[-1]])\n",
    "    # temp_validation_dir_x = os.listdir(validation_dir)[train_index[0]:train_index[-1]]\n",
    "    # temp_validation_dir_x.extend(os.listdir(validation_dir)[train_index[0]:train_index[-1]])\n",
    "   \n",
    "    labels = np.array([x[1] for x in temp_train])\n",
    "    images = np.array([x[0] for x in temp_train])\n",
    "    print(len(labels[~np.isnan(labels)]), len(labels))\n",
    "    print(len(images[~np.isnan(images)]), len(images))\n",
    "\n",
    "    images = images[~np.isnan(images)]\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(images ,labels , epochs=10, verbose=1, batch_size=20)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    score = model.score(temp_validate, verbose=0)\n",
    "\n",
    "    # Append the score to a list\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "# Calculate the mean score\n",
    "mean_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds\n",
    "k = 10\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=k)\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop over the folds\n",
    "for train_index, test_index in kf.split(training_data):\n",
    "    # Get the training and test sets for this fold\n",
    "\n",
    "    temp_train = np.array(training_data)[train_index]\n",
    "\n",
    "    temp_validate = np.array(training_data)[test_index]\n",
    "\n",
    "    labels = np.array([x[1] for x in temp_train])\n",
    "    images = np.array([x[0] for x in temp_train])\n",
    "\n",
    "    num_val_samples = len(labels)\n",
    "\n",
    "    print('processing ' + str(i+1) + '.fold')\n",
    "\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "                        [train_data[:i * num_val_samples],\n",
    "                         train_data[(i + 1) * num_val_samples:]],\n",
    "                        axis=0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "                        [train_targets[:i * num_val_samples],\n",
    "                         train_targets[(i + 1) * num_val_samples:]],\n",
    "                        axis=0)\n",
    "\n",
    "    # Build the model (each model should be build with the same parameters)\n",
    "    \n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    '''Write everything to the history object and explore it afterwards (see below)'''\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    score = model.score(temp_validate, verbose=0)\n",
    "\n",
    "    # Append the score to a list\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "# Calculate the mean score\n",
    "mean_score = np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
