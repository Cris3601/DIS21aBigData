{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ../data/abgabe_dir/all_pictures/malignant/767.jpg\n",
      "File not found: ../data/abgabe_dir/all_pictures/malignant/776.jpg\n",
      "File not found: ../data/abgabe_dir/all_pictures/malignant/788.jpg\n"
     ]
    }
   ],
   "source": [
    "# directory handling \n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "# assuming the data is in a subdirectory called \"data\" and fully unzipped\n",
    "# unzip the data.zip file in the data folder\n",
    "# if you are using the data.zip file from the moodle, you can use the following code to unzip it\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('../data/data.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('../data')\n",
    "\n",
    "# create a directory for the data if it does not exist\n",
    "pathlib.Path('data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create folder for ussage in the notebook\n",
    "pathlib.Path('data/abgabe_dir').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dir = '../data/abgabe_dir/train'\n",
    "validation_dir = '../data/abgabe_dir/validation'\n",
    "test_dir = '../data/abgabe_dir/test'\n",
    "\n",
    "all_data_in_one_folder = '../data/abgabe_dir/all_pictures'\n",
    "\n",
    "# copy the data to the new folder if data is not already there\n",
    "if(os.path.exists(all_data_in_one_folder) == False):\n",
    "    shutil.copytree('../data/train', all_data_in_one_folder, dirs_exist_ok=True)\n",
    "    shutil.copytree('../data/test', all_data_in_one_folder, dirs_exist_ok=True)\n",
    "\n",
    "# if(os.path.exists(train_dir) == False):\n",
    "#     shutil.copytree('../data/train', train_dir)\n",
    "\n",
    "# if(os.path.exists(validation_dir) == False):\n",
    "#     shutil.copytree('../data/test', validation_dir)\n",
    "\n",
    "# if(os.path.exists(test_dir) == False):\n",
    "#     shutil.copytree('../data/test', test_dir)\n",
    "\n",
    "# copy 1000 from all_data_in_one_folder/bengin to validation_dir/bengin\n",
    "\n",
    "\n",
    "def create_abgabe_path(label, size_of_train, size_of_validation, size_of_test):\n",
    "    train_dir_label = f\"../data/abgabe_dir/train/{label}\"\n",
    "    validation_dir_label = f\"../data/abgabe_dir/validation/{label}\"\n",
    "    test_dir_label = f\"../data/abgabe_dir/test/{label}\"\n",
    "\n",
    "    src_dir = f\"../data/abgabe_dir/all_pictures/{label}\"\n",
    "\n",
    "    # if not existst, create the folder\n",
    "    pathlib.Path(train_dir_label).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(validation_dir_label).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(test_dir_label).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    fnames = ['{}.jpg'.format(i) for i in range(1,size_of_train)]\n",
    "    for fname in fnames:\n",
    "        src = src_dir + \"/\" + fname\n",
    "        dst = train_dir_label + \"/\" + fname\n",
    "    \n",
    "        try:\n",
    "            shutil.copyfile(src, dst)\n",
    "        except:\n",
    "            print(\"File not found: \" + src)\n",
    "            \n",
    "\n",
    "    fnames = ['{}.jpg'.format(i) for i in range(size_of_train, size_of_train+size_of_validation)]\n",
    "    for fname in fnames:\n",
    "        src = src_dir + \"/\" + fname\n",
    "        dst = validation_dir_label + \"/\" + fname    \n",
    "        \n",
    "        try:\n",
    "            shutil.copyfile(src, dst)\n",
    "        except:\n",
    "            print(\"File not found: \" + src)\n",
    "            \n",
    "\n",
    "    fnames = ['{}.jpg'.format(i) for i in range(size_of_train + size_of_validation, size_of_train + size_of_validation + size_of_test)]\n",
    "    for fname in fnames:\n",
    "        src = src_dir + \"/\" + fname\n",
    "        dst = test_dir_label + \"/\" + fname\n",
    "\n",
    "        try:\n",
    "            shutil.copyfile(src, dst)\n",
    "        except:\n",
    "            print(\"File not found: \" + src)\n",
    "            \n",
    "\n",
    "create_abgabe_path(\"benign\", size_of_train=1000, size_of_validation=250, size_of_test=250)\n",
    "create_abgabe_path(\"malignant\", size_of_train=1000, size_of_validation=250, size_of_test=250)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "\n",
    "def build_basic_model(dropout=0):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            input_shape=(224, 224, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    # dropout \n",
    "    if dropout != 0:\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "# test_plan2[7] = {\"learning rate\": 2e-5, \"dropout\": 0, \"weight regularization\": 1e-1}\n",
    "\n",
    "def build_model_pretrained(n_units, learning_rate, weights=0, dropout=0):\n",
    "    \n",
    "\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(224, 224, 3))\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    if(weights > 0):\n",
    "        model.add(layers.Dense(n_units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=weights, l2=weights)))\n",
    "    else:\n",
    "        model.add(layers.Dense(n_units, activation='relu'))\n",
    "    \n",
    "    if(dropout > 0):\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "                  metrics=['acc'])\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3297 images belonging to 2 classes.\n",
      "the number of pictures in this dic is: 3297\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n"
     ]
    }
   ],
   "source": [
    "# richtiges coding\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "model = build_basic_model()\n",
    "\n",
    "# All images will be [0,1] standardized\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "k_fold_dir = \"../data/abgabe_dir/all_pictures/\"\n",
    "training_generator = train_datagen.flow_from_directory(\n",
    "    # This is the target directory\n",
    "    k_fold_dir,\n",
    "    # All images will be resized to 150x150\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    # Since binary_crossentropy loss is used, binary labels are needed\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "\n",
    "training_data = []\n",
    "\n",
    "# malignant\n",
    "max_value = (len(os.listdir(k_fold_dir+\"/benign\")) + len(os.listdir(k_fold_dir+\"/malignant\")))\n",
    "print(\"the number of pictures in this dic is:\",max_value)\n",
    "for index, x in enumerate(training_generator):\n",
    "    \n",
    "    # safe label info as integer\n",
    "    if(1 in x[1] ):\n",
    "        temp_to_add = 1\n",
    "    else:\n",
    "        temp_to_add = 0\n",
    "\n",
    "    # ingore the batch size and only take the first element of a list with one element\n",
    "    for i in x[0]:\n",
    "        training_data.append([i, temp_to_add])\n",
    "\n",
    "    if(index % max_value == 0 and index != 0):\n",
    "        break\n",
    "    if(index % 100 == 0):\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len(training_data[3200])\n",
    "\n",
    "# ebene 1 is the number of pictures \n",
    "# ebene 2 is malignant or benign\n",
    "# ebene 3 is the batch size - this makes problems\n",
    "# ebene 4 is the picture\n",
    "\n",
    "\n",
    "# plt.imshow(training_data[1997])\n",
    "\n",
    "# shows if malignant or benign\n",
    "# is one\n",
    "print(training_data[121][1])\n",
    "# is zero\n",
    "print(training_data[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = []\n",
    "\n",
    "for i in training_data:\n",
    "    dict_before_df = {}\n",
    "    dict_before_df[\"label\"] = i[1]\n",
    "    dict_before_df[\"image\"] = i[0]\n",
    "    df.append(dict_before_df)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bene\\AppData\\Local\\Temp/ipykernel_20924/1280737057.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp_train = np.array(training_data)[train_index]\n",
      "C:\\Users\\Bene\\AppData\\Local\\Temp/ipykernel_20924/1280737057.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  temp_validate = np.array(training_data)[test_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2968 2968\n",
      "446767104 2968\n",
      "Epoch 1/10\n",
      "  3/149 [..............................] - ETA: 3:37 - loss: 1.9828 - acc: 0.4500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20924/1280737057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Train the model on the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Evaluate the model on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds\n",
    "k = 10\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=k)\n",
    "scores = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop over the folds\n",
    "for train_index, test_index in kf.split(training_data):\n",
    "    # Get the training and test sets for this fold\n",
    "    \n",
    "    temp_train = np.array(training_data)[train_index]\n",
    "\n",
    "    temp_validate = np.array(training_data)[test_index]\n",
    "    # y_train, y_test = training_data[train_index][0], training_data[test_index][0]\n",
    "    \n",
    "    # temp_train_dir_x = os.listdir(train_dir+\"/benign\")[train_index[0]:train_index[-1]]\n",
    "    # temp_train_dir_x.extend(os.listdir(train_dir+\"/malignant\")[train_index[0]:train_index[-1]])\n",
    "    # temp_validation_dir_x = os.listdir(validation_dir)[train_index[0]:train_index[-1]]\n",
    "    # temp_validation_dir_x.extend(os.listdir(validation_dir)[train_index[0]:train_index[-1]])\n",
    "   \n",
    "    labels = np.array([x[1] for x in temp_train])\n",
    "    images = np.array([x[0] for x in temp_train])\n",
    "    print(len(labels[~np.isnan(labels)]), len(labels))\n",
    "    print(len(images[~np.isnan(images)]), len(images))\n",
    "\n",
    "    # images = images[~np.isnan(images)]\n",
    "\n",
    "    # Train the model on the training set\n",
    "    model.fit(images ,labels , epochs=10, verbose=1, batch_size=20)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    score = model.score(temp_validate, verbose=0)\n",
    "\n",
    "    # Append the score to a list\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "# Calculate the mean score\n",
    "mean_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
